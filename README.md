# Docker-inator

This project is a [minimal] boilerplate to generate containerized pipelines or services, reusing the docker image as possible to avoid duplicities.

It is orchestrated with an entry point and the parameters for each run might be passed using an env file.

The target of `Docker-inator` is to ease the deploy of services or pipelines in a normalized and frictionless way, so you can focus on the service own code. That being said, it's still probable that you'll need to customize the given files to fit your purpose.

Let's see how can we build and run `myservice`.

## Dev environment

This project uses [pre-commit hooks](https://pre-commit.com/) and [Commitizen](https://commitizen-tools.github.io/commitizen/), and it's recommended that any project using this boilerplate follows this pattern.

The configuration for both tools are located at the yaml files in the root of the project.

Let's start setting up the dev environment

```bash
./setup.sh
```

So, now you should commit & push using `cz` command instead of `git`

```bash
cz commit -a && git push
```

It will guide you to write a [conventional commit](https://www.conventionalcommits.org/en/v1.0.0/)

![img](https://commitizen-tools.github.io/commitizen/images/demo.gif)

You can still use `git commit`, but you will be forced to format the commit message to the **conventional commit** convention.

In order to bump the project version (both as a git tag and in the changelog), just

```bash
cz commit -a && \ # the commit to be tagged
cz bump --annotated-tag-message "hi, i'm the new tagged version of the project" && \ # the annotated tag
git push --follow-tags # push both the tagged commit and its tag
```

It will check the commits' type to follow semver convention in order to bump the version. The autogenerated changelog follows [keep a changelog](https://keepachangelog.com/en/1.1.0/) convention.

## Build

This folder contains everything needed to build a boilerplate docker image.

Copy this folder to your project folder and customize the files as needed.

### Dokerfile breakdown

Let's walk through the provided Dockerfile, line by line, in order to understand and be able to edit or delete any line, or add stuff as needed.

```dockerfile
FROM ubuntu:noble
```

^ This line set the image your building upon. You might want to use `postgis/postgis`, or maybe `osgeo/gdal:ubuntu-small-latest` or any other that fits your needs.

```dockerfile
ENV TZ=Europe/Madrid \
    DEBIAN_FRONTEND=noninteractive
```

^ Now, we set the timezone to avoid some questions, and disable interactivity while installing the stuff that might block the flow and exit with an error.

```dockerfile
COPY config/requirements.txt .
COPY config/packages.txt .
COPY config/package.json .
```

^ Copy the requirements (python dependencies) and packages (Debian/Ubuntu and nodeJS dependencies) files too.

```dockerfile
RUN apt update \
    && xargs -r -a packages.txt apt install -y \
    && apt clean
```

^ Let's install the Debian/Ubuntu dependencies if any.

```dockerfile
RUN apt install python3-pip -y \
    && pip install --no-cache-dir -r requirements.txt.
```

^ Let's install the python dependencies if any.

```dockerfile
RUN apt install nodejs -y \
    && npm install -g
```

^ Let's install the node modules if any. For specific NodeJS version, check [this](https://github.com/nodesource/distributions#installation-instructions)

```dockerfile
RUN apt clean
RUN rm -rf /var/lib/apt/lists/*
```

^ Housecleaning

### Set the image up

The only compulsory files to build the image are:

* `config/packages.txt` to define the eventual Debian/Ubuntu requirements
* `config/requirements.txt` to define the eventual python requirements
* `config/package.json` to define the eventual node modules

To set it up

* You might want to edit the `Dockerfile` and change the image you are importing from.
* Edit the `config/packages.txt` file to add the eventual **apt** packages needed to run your service. An empty packages.txt file is allowed.
* Edit the `config/requirements.txt` file to add the eventual **python** packages needed to run your service. An empty requirements.txt file is allowed.
* Edit the `config/package.json` file to add the eventual **node** modules needed to run your service. If none, delete the `dependencies` block within the json file.

If you want to run a node module as an executable, just use `npx my_module` within the entrypoint or anywhere else in your code.

### Compile it

Sugar coated `build.sh` script is provided, edit it as needed. Then run

```bash
./build.sh myserviceimage
```

Now you have a docker image named `myserviceimage` available

## Run

This folder contains sample code and structure that might help you build your own service.

Let's say your service needs two folders:

* `src`, where the code lives
* `data`, where the input/output/tmp files are located

We need an `entrypoint.sh` file to start our service. You can locate it in the same path defined at the dockerfile, or override the path at run time. We'll get back to this later.

Following the default dockerfile, a minimal project structure would be:

* config/
  * config.env
* src/
  * entrypoint.sh
* data/

The env file might be generated and passed at runtime, so its path here is just for the sake of the explanation.

### Service template

The `run` folder contains a minimal template to build services or pipelines that require:

* Runtime env values
* The option to run the whole pipeline or just a subset of steps within
* Access to host to share files

#### config.env

This environment file will be used to set each run up. Any bash data type is accepted, and arrays are expressed as `colon-separated strings`, like `es:it:pt`

The only compulsory parameter here (for this template) is `STEPS`, that will be use to choose the steps and order to be run. The accepted values are:

* `all`: all the steps will be run, in the order they re found in the entreypoint script
* `tag`: the tag of the unique step to be run, it can be an ordinal or a text label
* `tag1:tag2:tag3`: the set of steps and the order to be run

#### entrypoint.sh

This script will orchestrate the execution of the server, and its minimal version should look like

```bash
#!/bin/bash

tstamp() { date +[%T:%N]; }

echo "$(tstamp) - START"

# GLOBALS
export -f tstamp
set -a
    myglobal=myvalue
set +a

IFS=':' read -r -a sarray <<< "$STEPS"

# Steps block
step=1
if [[ " ${sarray[@]} " =~ " ${step} " || " ${sarray[@]} " =~ " all " ]]
then
    echo "$(tstamp) - Step $step: Step 1 description"
    source ./bash/step_01.sh
else
    echo "$(tstamp) - Skipping step $step"
fi

step=2
if [[ " ${sarray[@]} " =~ " ${step} " || " ${sarray[@]} " =~ " all " ]]
then
    echo "$(tstamp) - Step $step: Step 2 description"
    source ./bash/step_02.sh
else
    echo "$(tstamp) - Skipping step $step"
fi

# Other steps..

# Done
echo "$(tstamp) - FINISH"

```

So each step is run if its tag is present in the `STEPS` env variable

##### Step scripts

Each step block calls `step_mytag.sh` script in order to orchestrate the tasks related to the step itself.

In case the step requires to run a SQL script agains a DB, you can use all the global variables within the SQL script file, just using common bash syntax (vg.: `select * from schema.$TABLE where country = $COUNTRY`), and then running the script like

```bash
cat sql/step_{tag}.sql | envsubst | db_client db_connection_string

```

If you are using Postgres, you can pass aditional parameters using `-v` option, and then using the parameter within the script with a colon prefix (vg.:`select * from schema.$table where country = $COUNTRY and date=:mydate` ) and then running the script the script like

```bash
cat sql/step_{tag}.sql | envsubst | \
PGPASSWORD=$PG_PWD psql -h $PG_SERVER -p $PG_PORT -d $db -U $PG_USER -q \
-v mydate='2025/12/02' 

```

### Run it

You just need to run

```bash
./run.sh myserviceimage myservice
```

This script will run the following command

```bash
docker run -ti --rm \
--name myservice \
-v $(pwd)/src:/src \
-v $(pwd)/data:/data \
--net=host \
-w /src/ \
--env-file $(pwd)/config/config.env \
--entrypoint $(pwd)/src/entrypoint.sh \
myserviceimage &> myservice.log

```

So it will:

* Run a container called `myservice` using the image `myserviceimage`
* The container will run in the foreground and keep the interactivity meanwhile
* The container will be destroyed once the pipeline finish
* Map the `./src` and `./data` folders to host folders
* Use the host network interface
* Set the `/src` folder as the working dir, this means that any relative path will be start here.
* Get the config from an env file located at `./config/config.env`
* Override the path of the entrypoint defined in the dockerfile to my `/src/entrypoint.sh` file (from the point of view of the container, once the folders are mapped)
* Both stdout and stderror will be piped to `myservice`.log

Do you want it to run in the background? Then you should execute

```bash
docker run -ti -d \
--name myservice \
-v $(pwd)/src:/src \
-v $(pwd)/data:/data \
--net=host \
--restart=unless-stopped \
-w /src/ \
--env-file $(pwd)/config/config.env \
--entrypoint $(pwd)/src/entrypoint.sh \
myserviceimage &> myservice.log

```

So it will run the container in the background (detached) and restart it if anything fails, unless you stop it on purpose.

And you will be able to attach/detach to the service just by using

```bash
docker attach myservice

```

Just `ctrl + c` to detach once you've finished your business with `myservice`

## Deploy and run remotely

This script in the `remote_run` folder lets you deploy, build and run your service in remote instance and in an unattended and detached way. It's provided as sugarsyntax, just to help you run your work remotely without any friction.

It just needs a SSH connection with the remote host.

`remoterun.sh` script expects the same folder structure of this repo, and the code to be run located at `run` folder. So, you might need to edit it.

Edit `config_remote.env` to fit your needs

```bash
HOST=*******************************
USER=*******************************
PASSWORD=*****************************
IMAGENAME=***************************
SERVICENAME=myservice
```

Then just

```bash
./remoterun.sh
```

To check the status of the service you can connect to the remote session just by using [tmux](https://github.com/tmux/tmux)

```bash
tmux attach-session -t myservice
```

And, to detach from remote session:

```bash
tmux detach
```
