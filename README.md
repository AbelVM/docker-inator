# Docker-inator

This project is a [minimal] boilerplate to generate containerized processes.

It is orchestrated with an entry point and the parameters for each run are passed using an env file.

Let's see how can we build and run `myservice`.

## Dev enviroment

This project uses [pre-commit hooks](https://pre-commit.com/) and [Commitizen](https://commitizen-tools.github.io/commitizen/), and it's recommended that any project using this boilerplate follows this pattern.

Let's start setting up the dev environment

```bash
./setup.sh
```

So, now you commit using the following command instead of `git commit`

```bash
cz commit && git push
```

It will guide you to write a [conventional commit](https://www.conventionalcommits.org/en/v1.0.0/)

![img](https://commitizen-tools.github.io/commitizen/images/demo.gif)

You can still use `git commit`, but you will be forced to format the commit message to the **conventional commit** convention.

In order to bump the project version (both as a git tag and in the changelog), just

```bash
cz bump --annotated-tag-message "him, i'm the new tagged version of the project" && git push --tags
```

It will check the commits' type to follow semver convention in order to bump the version. The autogenerated changelog follows [keep a changelog](https://keepachangelog.com/en/1.1.0/) convention.

## Project structure

### config.env

This environment fill will be used to set each run up. Any bash data type is accepted, and arrays are expresed as `colon-separated strings`, like `es:it:pt`

The only compulsory parameter here is `STEPS`, that will be use to choose the steps and order to be run. The accepted values are:

* `all`: all the steps will be run, in the order they re found in the entreypoint script
* `tag`: the tag of the unique step to be run
* `tag1:tag2:tag3`: the set of steps and the order to be run

### entrypoint.sh

This script will orchestrate the execution of the server, and its minimal version should look like

```bash
#!/bin/bash

tstamp() { date +[%T:%N]; }

echo "$(tstamp) - START"

# GLOBALS
export -f tstamp
set -a
    myglobal=myvalue
set +a

IFS=':' read -r -a sarray <<< "$STEPS"

# Steps block
step={tag}
if [[ " ${sarray[@]} " =~ " ${step} " || " ${sarray[@]} " =~ " all " ]]
then
    echo "$(tstamp) - Step $step: Step description"
    source ./bash/step_{tag}.sh
else
    echo "$(tstamp) - Skipping step $step"
fi

# ...

# Done
echo "$(tstamp) - FINISH"

```

So each step is run if its tag is present in the `STEPS` env variable

### step_{tag}

Each step bloc calls `step_{tag}.sh` script in order to orchestrate the tasks related to the step itself.

In case the step requires to run a SQL script agains a DB, you can use all the global variables within the SQL script file, just using common bash syntax (vg.: `select * from schema.$table where country = $country`), and then running the script like

```bash
cat sql/step_{tag}.sql | envsubst | db_client db_connection_string

```

If you are using Postgres, you can pass aditional parameters using `-v` option, and then using the parameter within the script with a colon prefix (vg.:`select * from schema.$table where country = $country and date=:mydate` ) and then running the script the script like

```bash
cat sql/step_{tag}.sql | envsubst | \
PGPASSWORD=$PG_PWD psql -h $PG_SERVER -p $PG_PORT -d $db -U $PG_USER -q \
-v myparam1=test1 -v myparam2=test2

```

## Dokerfile breakdown

Let's walk through the provided Dockerfile, line by line, in order to understand and be able to edit or delete any line, or add stuff as needed.

```dockerfile
FROM ubuntu:jammy
```

^ This line set the image your building upon. You might want to use `postgis/postgis`, or maybe `osgeo/gdal:ubuntu-small-latest` or any other that fits your needs.

```dockerfile
ENV TZ=Europe/Madrid \
    DEBIAN_FRONTEND=noninteractive
```

^ Now, we set the timezone to avoid some questions, and disable interactivity while installing the stuff that might block the flow and exit with an error.

```dockerfile
COPY src/ /src/
RUN find ./src -name "*.sh" -exec chmod +x {} +
```

^ Let's copy the source folder to the container! And make executable all the `.sh` files if any.

```dockerfile
COPY config/requirements.txt .
COPY config/packages.txt .
COPY config/package.json .
```

^ Copy the requirements (python dependencies) and packages (Debian/Ubuntu dependencies) files too.

```dockerfile
RUN apt update \
    && xargs -r -a packages.txt apt install -y \
    && apt clean
```

^ Let's install the Debian/Ubuntu dependencies if any.

```dockerfile
RUN apt install python3-pip -y \
    && pip install --no-cache-dir -r requirements.txt.
```

^ Let's install the python dependencies if any.

```dockerfile
RUN apt install nodejs -y \
    && npm install -g
```

^ Let's install the node modules if any. For specific NodeJS version, check [this](https://github.com/nodesource/distributions#installation-instructions)

```dockerfile
RUN mkdir ./src/data
```

^ Creating the `data` folder within `/src`.

```dockerfile
WORKDIR /src
```

^ This line sets the `src` folder as the working dir, this means that any relative path will be start here. The host's data folder will be mapped to `/src/data/` so it will be accessible from yur code with a relativa path like `data/`.

```dockerfile
ENTRYPOINT ["/src/entrypoint.sh"]
```

^ This final line set the executable script that will be run any time you perform `docker run ... myservice`.

Sugar coated `build` and `run` scripts are provided, just edit them as needed.

### Set it up

The only compulsory files to build the image are:

* `src/entrypoint.sh` to start and, optionally, orchestrate, the service.
* `config/packages.txt` to define the eventual Debia/Ubuntu requirements
* `config/requirements.txt` to define the eventual python requirements
* `config/package.json` to define the eventual node modules

To set it up

* Edit the `src/entrypoint.sh` and add the required files in `/src` and `/data` to fulfill the requirements of your project.
* You might want to edit the `Dockerfile` and change the image you are importing from.
* Edit the `config/packages.txt` file to add the eventual **apt** packages needed to run your service. An empty packages.txt file is allowed.
* Edit the `config/requirements.txt` file to add the eventual **python** packages needed to run your service. An empty requirements.txt file is allowed.
* Edit the `config/package.json` file to add the eventual **node** modules needed to run your service. If none, delete the `dependencies` block within the json file.

If you want to run a node module as an executable, just use `npx my_module` within the entrypoint or anywhere else in your code.

### Build it

The `src` folder contains sample code and structure that might help you build your own service.

The working dir will be this very `src` folder, so take that into account to define al relative paths that you might need.

A folder called `data` is expected to exist in the root of the project and it will be the one mapped to the container, with the same name.

Minimal project structure:

* src
  * entrypoint.sh
* data

Equivalent folder structure within the container

* src
  * data
  * entrypoint.sh

### Compile it

First, just

```bash
./build.sh myservice
```

### Run it

You need to set the environment file up, in order to define the parameters to run the container

* Copy `config/config.env.template` to `config/config.env` and edit it to meet your settings.
  * Arrays are declared as `item1:item2:item3`.
  * `STEPS` is the array parameter that defines the named steps that will be run, being `all` the defined value to perform a full run.

Then, just...

```bash
./run.sh myservice
```

Both stdoutput and stderror will be piped to `myservice`.log.

### Deploy remotely!

This script lets you deploy, build and run your service in an unattended and dettached way

Edit `remoterun.sh` to fit your needs

```bash
HOST=*******************************
USER=*******************************
PASSWORD=*****************************
SERVICENAME=***************************
```

Then just

```bash
./remoterun.sh
```

To check the status of the service you can connect to the remote session just by

```bash
tmux attach-session -t "$SERVICENAME"
```

And, to dettach from remote session:

```bash
tmux detach
```
